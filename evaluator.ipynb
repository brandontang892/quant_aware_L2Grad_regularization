{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {},
    "id": "jUNeL-QVYC6R"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import sys\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from models import ConvNet\n",
    "from utils import parse_args, make_criterion\n",
    "\n",
    "\n",
    "# training function\n",
    "def train(net, trainloader, criterion, optimizer, epoch, lmbda, train_loss_tracker, train_acc_tracker):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs.requires_grad=True\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "#         if lbda == 0:\n",
    "#             loss = criterion(outputs, targets)     #nn.CrossEntropyLoss(outputs, targets) # Add regularization term (define new class)\n",
    "#         elif lbda > 0:\n",
    "#             loss = criterion(outputs, targets, net)\n",
    "#         else:\n",
    "#             NotImplementedError \n",
    "        loss = criterion(outputs, targets, net)\n",
    "        loss.backward()\n",
    "        # update optimizer state\n",
    "        optimizer.step()\n",
    "        # compute average loss\n",
    "        train_loss += loss.item()\n",
    "        train_loss_tracker.append(loss.item())\n",
    "        loss = train_loss / (batch_idx + 1)\n",
    "        # compute accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100. * correct / total\n",
    "        # Print status\n",
    "        sys.stdout.write(f'\\rEpoch {epoch}: Train Loss: {loss:.3f}' +  \n",
    "                         f'| Train Acc: {acc:.3f}')\n",
    "        sys.stdout.flush()\n",
    "    train_acc_tracker.append(acc)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# testing function \n",
    "def test(net, testloader, criterion, epoch, lmbda, test_loss_tracker, test_acc_tracker):\n",
    "    global best_acc\n",
    "    best_acc = 0 \n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "#             if lbda == 0:\n",
    "#                 loss = criterion(outputs, targets)     #nn.CrossEntropyLoss(outputs, targets) # Add regularization term (define new class)\n",
    "#             elif lbda > 0:\n",
    "#                 loss = criterion(outputs, targets, net)\n",
    "#             else:\n",
    "#                 NotImplementedError \n",
    "        loss = criterion(outputs, targets, net)\n",
    "        test_loss += loss.item()\n",
    "        test_loss_tracker.append(loss.item())\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        loss = test_loss / (batch_idx + 1)\n",
    "        acc = 100.* correct / total\n",
    "    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    test_acc_tracker.append(acc)\n",
    "    if acc > best_acc:\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc  \n",
    "\n",
    "\n",
    "def quantizer(input, nbit):\n",
    "    '''\n",
    "    input: full precision tensor in the range [0, 1]\n",
    "    return: quantized tensor\n",
    "    '''\n",
    "    output = input * (2**nbit -1)\n",
    "    output = torch.round(output)\n",
    "\n",
    "    return output/(2**nbit -1)\n",
    "\n",
    "\n",
    "def dorefa_g(w, nbit, adaptive_scale=None):\n",
    "    '''\n",
    "    w: a floating-point weight tensor to quantize\n",
    "    nbit: the number of bits in the quantized representation\n",
    "    adaptive_scale: the maximum scale value. if None, it is set to be the\n",
    "                    absolute maximum value in w.\n",
    "    '''\n",
    "    if adaptive_scale is None:\n",
    "        adaptive_scale = torch.max(torch.abs(w))\n",
    "\n",
    "    # Part 3.2: Implement based on stochastic quantization function above\n",
    "    # basically, quantize and dequantize (with added noise in the middle)\n",
    "    noise_tensor = (torch.rand(w.shape, device=w.device) - 0.5) / (2**nbit - 1)\n",
    "    intermediate = quantizer(noise_tensor + 0.5 + w / (2*adaptive_scale), nbit)\n",
    "    w_q = 2 * adaptive_scale * (intermediate - 0.5)\n",
    "\n",
    "    # remove placeholder \"return w, adaptive_scale\" line below \n",
    "    # after you implement\n",
    "    return w_q, adaptive_scale\n",
    "\n",
    "\n",
    "def quantize_model(model, nbit):\n",
    "    '''\n",
    "    Used to quantize the ConvNet model\n",
    "    '''\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            m.weight.data, m.adaptive_scale = dorefa_g(m.weight, nbit)\n",
    "            if m.bias is not None:\n",
    "                m.bias.data,_ = dorefa_g(m.bias, nbit, m.adaptive_scale)\n",
    "\n",
    "\n",
    "# Load training data\n",
    "transform_train = transforms.Compose([                                   \n",
    "    transforms.RandomCrop(32, padding=4),                                       \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# Load testing data\n",
    "transform_test = transforms.Compose([                                           \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
    "\n",
    "\n",
    "# python script\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    \n",
    "    # seed\n",
    "    SEED = args.seed\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    # unpack args\n",
    "    device = args.device\n",
    "    epochs = args.n_epochs\n",
    "    #regulquant_on = args.regulquant\n",
    "    lmbda = args.lmbda\n",
    "    \n",
    "    net = ConvNet()\n",
    "    net = net.to(device)\n",
    "    \n",
    "    lr = args.lr\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = None\n",
    "    criterion = make_criterion(args)\n",
    "    \n",
    "    \n",
    "    train_loss_tracker, train_acc_tracker = [], []\n",
    "    test_loss_tracker, test_acc_tracker = [], []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(0, epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # call train function\n",
    "        train(net, trainloader, criterion, optimizer, epoch, lmbda, train_loss_tracker, train_acc_tracker)\n",
    "        \n",
    "        # call test function\n",
    "        test(net, testloader, criterion, epoch, lmbda, test_loss_tracker, test_acc_tracker)\n",
    "        \n",
    "        # scheduler step\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_total_time = epoch_end_time - epoch_start_time        \n",
    "        print(f\"Epoch runtime: {epoch_total_time}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print('Total runtime: {} seconds'.format(total_time))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {},
    "id": "xGAZiIrIYGNG"
   },
   "outputs": [],
   "source": [
    "n_bits = 8\n",
    "quantize_model(net, n_bits)\n",
    "test(net, testloader, criterion, epoch, lmbda, test_loss_tracker, test_acc_tracker)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "evaluator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
