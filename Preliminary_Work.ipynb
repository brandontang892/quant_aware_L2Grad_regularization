{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import OrderedDict\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "transform_train = transforms.Compose([                                   \n",
    "    transforms.RandomCrop(32, padding=4),                                       \n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                        download=True,\n",
    "                                        transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "# Load testing data\n",
    "transform_test = transforms.Compose([                                           \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True,\n",
    "                                       transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# CIFAR model (architecture from CS 242)\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1,\n",
    "               padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding,\n",
    "                  bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            conv_block(3, 32),\n",
    "            conv_block(32, 32),\n",
    "            conv_block(32, 64, stride=2),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 64),\n",
    "            conv_block(64, 128, stride=2),\n",
    "            conv_block(128, 128),\n",
    "            conv_block(128, 256),\n",
    "            conv_block(256, 256),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.model(x)\n",
    "        B, C, _, _ = h.shape\n",
    "        h = h.view(B, C)\n",
    "        return self.classifier(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "def train(net, epoch, train_loss_tracker, train_acc_tracker):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets) # Add regularization term (define new class)\n",
    "        loss.backward()\n",
    "        # update optimizer state\n",
    "        optimizer.step()\n",
    "        # compute average loss\n",
    "        train_loss += loss.item()\n",
    "        train_loss_tracker.append(loss.item())\n",
    "        loss = train_loss / (batch_idx + 1)\n",
    "        # compute accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        acc = 100. * correct / total\n",
    "        # Print status\n",
    "        sys.stdout.write(f'\\rEpoch {epoch}: Train Loss: {loss:.3f}' +  \n",
    "                         f'| Train Acc: {acc:.3f}')\n",
    "        sys.stdout.flush()\n",
    "    train_acc_tracker.append(acc)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def test(net, epoch, test_loss_tracker, test_acc_tracker):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    best_acc = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            test_loss_tracker.append(loss.item())\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            loss = test_loss / (batch_idx + 1)\n",
    "            acc = 100.* correct / total\n",
    "    sys.stdout.write(f' | Test Loss: {loss:.3f} | Test Acc: {acc:.3f}\\n')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    test_acc_tracker.append(acc)\n",
    "    if acc > best_acc:\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir('checkpoint'):\n",
    "            os.mkdir('checkpoint')\n",
    "        torch.save(state, './checkpoint/ckpt.pth')\n",
    "        best_acc = acc  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-9-5ad4b4b5825b>, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-5ad4b4b5825b>\"\u001b[0;36m, line \u001b[0;32m18\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def regularizer(net, discretizer, distance='KL'):\n",
    "    '''\n",
    "    Given a net, creates a penalty based on the distance of the net's weight \n",
    "    distribution and the input distribution \n",
    "    Args: \n",
    "        net: model\n",
    "        discretizer (int): (positive) number of bins to simplify weight disribution\n",
    "    '''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "            w = m.weight; b = m.bias     # usually bias will be zero (we init conv blocks as bias=False)\n",
    "            \n",
    "            \n",
    "    \n",
    "    if distance == 'KL':\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "class UniformRegularizingLoss(nn.Module):\n",
    "    def __init__(self, lbda, distribution='uniform', weighted=None):\n",
    "        super(RegularizingLoss, self).__init__()\n",
    "        \n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction=\"mean\", weight=weighted)\n",
    "        self.lbda = lbda\n",
    "        #self.dist = distribution\n",
    "        \n",
    "    def forward(self, logits, labels, net, bins=30):\n",
    "        regularizing_term = regularizer(net, discretizer=bins)\n",
    "        \n",
    "        return self.lbda * regularizing_term + self.ce_loss(logits, labels)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gradient": {},
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100 epochs, with learning rate 0.1 and milestones [25, 50, 75, 100]\n",
      "Epoch 0: Train Loss: 1.630| Train Acc: 38.790 | Test Loss: 1.380 | Test Acc: 48.900\n",
      "Epoch 0: 11.352970361709595 seconds\n",
      "Epoch 1: Train Loss: 1.147| Train Acc: 58.532 | Test Loss: 1.090 | Test Acc: 61.620\n",
      "Epoch 1: 11.700154781341553 seconds\n",
      "Epoch 2: Train Loss: 0.897| Train Acc: 68.368 | Test Loss: 1.526 | Test Acc: 55.240\n",
      "Epoch 2: 11.708998680114746 seconds\n",
      "Epoch 3: Train Loss: 0.764| Train Acc: 73.256 | Test Loss: 1.148 | Test Acc: 66.000\n",
      "Epoch 3: 11.338812112808228 seconds\n",
      "Epoch 4: Train Loss: 0.697| Train Acc: 76.002 | Test Loss: 0.929 | Test Acc: 69.700\n",
      "Epoch 4: 11.356273889541626 seconds\n",
      "Epoch 5: Train Loss: 0.649| Train Acc: 77.598 | Test Loss: 1.101 | Test Acc: 64.110\n",
      "Epoch 5: 11.651323080062866 seconds\n",
      "Epoch 6: Train Loss: 0.611| Train Acc: 78.742 | Test Loss: 0.849 | Test Acc: 71.820\n",
      "Epoch 6: 11.755674123764038 seconds\n",
      "Epoch 7: Train Loss: 0.579| Train Acc: 80.030 | Test Loss: 0.894 | Test Acc: 70.960\n",
      "Epoch 7: 11.511911392211914 seconds\n",
      "Epoch 8: Train Loss: 0.560| Train Acc: 80.814 | Test Loss: 0.686 | Test Acc: 77.160\n",
      "Epoch 8: 11.42121934890747 seconds\n",
      "Epoch 9: Train Loss: 0.547| Train Acc: 81.288 | Test Loss: 0.880 | Test Acc: 70.480\n",
      "Epoch 9: 11.612345218658447 seconds\n",
      "Epoch 10: Train Loss: 0.528| Train Acc: 81.802 | Test Loss: 0.725 | Test Acc: 75.440\n",
      "Epoch 10: 12.321795463562012 seconds\n",
      "Epoch 11: Train Loss: 0.516| Train Acc: 82.336 | Test Loss: 0.842 | Test Acc: 73.270\n",
      "Epoch 11: 11.749403238296509 seconds\n",
      "Epoch 12: Train Loss: 0.508| Train Acc: 82.586 | Test Loss: 0.695 | Test Acc: 77.020\n",
      "Epoch 12: 11.63357424736023 seconds\n",
      "Epoch 13: Train Loss: 0.493| Train Acc: 83.192 | Test Loss: 0.627 | Test Acc: 78.740\n",
      "Epoch 13: 12.05279278755188 seconds\n",
      "Epoch 14: Train Loss: 0.490| Train Acc: 83.084 | Test Loss: 0.900 | Test Acc: 73.030\n",
      "Epoch 14: 11.361161470413208 seconds\n",
      "Epoch 15: Train Loss: 0.478| Train Acc: 83.642 | Test Loss: 0.659 | Test Acc: 78.570\n",
      "Epoch 15: 11.649917364120483 seconds\n",
      "Epoch 16: Train Loss: 0.473| Train Acc: 83.774 | Test Loss: 0.581 | Test Acc: 80.980\n",
      "Epoch 16: 11.530439376831055 seconds\n",
      "Epoch 17: Train Loss: 0.462| Train Acc: 84.214 | Test Loss: 0.789 | Test Acc: 74.510\n",
      "Epoch 17: 11.656939029693604 seconds\n",
      "Epoch 18: Train Loss: 0.457| Train Acc: 84.386 | Test Loss: 0.836 | Test Acc: 74.070\n",
      "Epoch 18: 11.661369800567627 seconds\n",
      "Epoch 19: Train Loss: 0.457| Train Acc: 84.136 | Test Loss: 0.676 | Test Acc: 78.120\n",
      "Epoch 19: 12.08356523513794 seconds\n",
      "Epoch 20: Train Loss: 0.445| Train Acc: 84.684 | Test Loss: 0.578 | Test Acc: 80.960\n",
      "Epoch 20: 12.412691354751587 seconds\n",
      "Epoch 21: Train Loss: 0.441| Train Acc: 84.828 | Test Loss: 0.697 | Test Acc: 76.880\n",
      "Epoch 21: 11.795171022415161 seconds\n",
      "Epoch 22: Train Loss: 0.440| Train Acc: 84.842 | Test Loss: 0.701 | Test Acc: 78.560\n",
      "Epoch 22: 11.738521337509155 seconds\n",
      "Epoch 23: Train Loss: 0.436| Train Acc: 85.030 | Test Loss: 0.537 | Test Acc: 82.460\n",
      "Epoch 23: 11.782727718353271 seconds\n",
      "Epoch 24: Train Loss: 0.431| Train Acc: 85.362 | Test Loss: 0.617 | Test Acc: 79.050\n",
      "Epoch 24: 12.056490898132324 seconds\n",
      "Epoch 25: Train Loss: 0.278| Train Acc: 90.702 | Test Loss: 0.298 | Test Acc: 89.830\n",
      "Epoch 25: 12.256224870681763 seconds\n",
      "Epoch 26: Train Loss: 0.235| Train Acc: 92.048 | Test Loss: 0.291 | Test Acc: 89.890\n",
      "Epoch 26: 12.685379266738892 seconds\n",
      "Epoch 27: Train Loss: 0.214| Train Acc: 92.724 | Test Loss: 0.291 | Test Acc: 90.300\n",
      "Epoch 27: 12.091461181640625 seconds\n",
      "Epoch 28: Train Loss: 0.199| Train Acc: 93.286 | Test Loss: 0.281 | Test Acc: 90.560\n",
      "Epoch 28: 12.218915700912476 seconds\n",
      "Epoch 29: Train Loss: 0.187| Train Acc: 93.702 | Test Loss: 0.284 | Test Acc: 90.580\n",
      "Epoch 29: 11.62619948387146 seconds\n",
      "Epoch 30: Train Loss: 0.180| Train Acc: 93.852 | Test Loss: 0.281 | Test Acc: 90.750\n",
      "Epoch 30: 11.542675018310547 seconds\n",
      "Epoch 31: Train Loss: 0.170| Train Acc: 94.228 | Test Loss: 0.287 | Test Acc: 90.140\n",
      "Epoch 31: 11.582647323608398 seconds\n",
      "Epoch 32: Train Loss: 0.161| Train Acc: 94.620 | Test Loss: 0.289 | Test Acc: 90.670\n",
      "Epoch 32: 11.541822671890259 seconds\n",
      "Epoch 33: Train Loss: 0.157| Train Acc: 94.634 | Test Loss: 0.293 | Test Acc: 90.280\n",
      "Epoch 33: 11.561625719070435 seconds\n",
      "Epoch 34: Train Loss: 0.149| Train Acc: 94.878 | Test Loss: 0.291 | Test Acc: 90.470\n",
      "Epoch 34: 11.285136938095093 seconds\n",
      "Epoch 35: Train Loss: 0.143| Train Acc: 95.198 | Test Loss: 0.287 | Test Acc: 90.800\n",
      "Epoch 35: 11.867278814315796 seconds\n",
      "Epoch 36: Train Loss: 0.136| Train Acc: 95.364 | Test Loss: 0.298 | Test Acc: 90.390\n",
      "Epoch 36: 11.720202684402466 seconds\n",
      "Epoch 37: Train Loss: 0.134| Train Acc: 95.448 | Test Loss: 0.291 | Test Acc: 90.490\n",
      "Epoch 37: 11.57081389427185 seconds\n",
      "Epoch 38: Train Loss: 0.132| Train Acc: 95.474 | Test Loss: 0.311 | Test Acc: 90.070\n",
      "Epoch 38: 11.515361309051514 seconds\n",
      "Epoch 39: Train Loss: 0.129| Train Acc: 95.616 | Test Loss: 0.298 | Test Acc: 90.640\n",
      "Epoch 39: 11.591999530792236 seconds\n",
      "Epoch 40: Train Loss: 0.124| Train Acc: 95.804 | Test Loss: 0.326 | Test Acc: 89.880\n",
      "Epoch 40: 11.78429627418518 seconds\n",
      "Epoch 41: Train Loss: 0.122| Train Acc: 95.896 | Test Loss: 0.335 | Test Acc: 89.710\n",
      "Epoch 41: 11.588914632797241 seconds\n",
      "Epoch 42: Train Loss: 0.116| Train Acc: 96.082 | Test Loss: 0.299 | Test Acc: 90.850\n",
      "Epoch 42: 12.164333581924438 seconds\n",
      "Epoch 43: Train Loss: 0.115| Train Acc: 96.116 | Test Loss: 0.316 | Test Acc: 90.470\n",
      "Epoch 43: 12.883914232254028 seconds\n",
      "Epoch 44: Train Loss: 0.114| Train Acc: 96.132 | Test Loss: 0.331 | Test Acc: 90.090\n",
      "Epoch 44: 12.541878700256348 seconds\n",
      "Epoch 45: Train Loss: 0.112| Train Acc: 96.252 | Test Loss: 0.350 | Test Acc: 89.520\n",
      "Epoch 45: 12.317940473556519 seconds\n",
      "Epoch 46: Train Loss: 0.111| Train Acc: 96.256 | Test Loss: 0.340 | Test Acc: 89.640\n",
      "Epoch 46: 11.407938718795776 seconds\n",
      "Epoch 47: Train Loss: 0.109| Train Acc: 96.382 | Test Loss: 0.327 | Test Acc: 89.620\n",
      "Epoch 47: 11.419289827346802 seconds\n",
      "Epoch 48: Train Loss: 0.107| Train Acc: 96.374 | Test Loss: 0.334 | Test Acc: 90.010\n",
      "Epoch 48: 11.58459758758545 seconds\n",
      "Epoch 49: Train Loss: 0.113| Train Acc: 96.220 | Test Loss: 0.353 | Test Acc: 89.330\n",
      "Epoch 49: 11.60960078239441 seconds\n",
      "Epoch 50: Train Loss: 0.075| Train Acc: 97.692 | Test Loss: 0.266 | Test Acc: 91.560\n",
      "Epoch 50: 11.434310913085938 seconds\n",
      "Epoch 51: Train Loss: 0.062| Train Acc: 98.224 | Test Loss: 0.265 | Test Acc: 91.780\n",
      "Epoch 51: 11.334355354309082 seconds\n",
      "Epoch 52: Train Loss: 0.056| Train Acc: 98.482 | Test Loss: 0.262 | Test Acc: 91.880\n",
      "Epoch 52: 11.36482310295105 seconds\n",
      "Epoch 53: Train Loss: 0.053| Train Acc: 98.530 | Test Loss: 0.265 | Test Acc: 91.820\n",
      "Epoch 53: 11.46032166481018 seconds\n",
      "Epoch 54: Train Loss: 0.050| Train Acc: 98.730 | Test Loss: 0.265 | Test Acc: 91.940\n",
      "Epoch 54: 11.649013042449951 seconds\n",
      "Epoch 55: Train Loss: 0.048| Train Acc: 98.766 | Test Loss: 0.267 | Test Acc: 92.010\n",
      "Epoch 55: 12.24770212173462 seconds\n",
      "Epoch 56: Train Loss: 0.046| Train Acc: 98.8800 | Test Loss: 0.270 | Test Acc: 91.920\n",
      "Epoch 56: 11.38452434539795 seconds\n",
      "Epoch 57: Train Loss: 0.046| Train Acc: 98.8400 | Test Loss: 0.268 | Test Acc: 91.980\n",
      "Epoch 57: 11.549992561340332 seconds\n",
      "Epoch 58: Train Loss: 0.044| Train Acc: 98.8940 | Test Loss: 0.264 | Test Acc: 92.070\n",
      "Epoch 58: 12.442740440368652 seconds\n",
      "Epoch 59: Train Loss: 0.044| Train Acc: 98.898 | Test Loss: 0.266 | Test Acc: 92.200\n",
      "Epoch 59: 12.308514595031738 seconds\n",
      "Epoch 60: Train Loss: 0.042| Train Acc: 98.996 | Test Loss: 0.267 | Test Acc: 92.020\n",
      "Epoch 60: 11.734380006790161 seconds\n",
      "Epoch 61: Train Loss: 0.041| Train Acc: 99.032 | Test Loss: 0.265 | Test Acc: 92.320\n",
      "Epoch 61: 12.292134761810303 seconds\n",
      "Epoch 62: Train Loss: 0.039| Train Acc: 99.1240 | Test Loss: 0.262 | Test Acc: 92.160\n",
      "Epoch 62: 11.826997518539429 seconds\n",
      "Epoch 63: Train Loss: 0.039| Train Acc: 99.104 | Test Loss: 0.266 | Test Acc: 92.240\n",
      "Epoch 63: 11.557246446609497 seconds\n",
      "Epoch 64: Train Loss: 0.036| Train Acc: 99.204 | Test Loss: 0.263 | Test Acc: 92.290\n",
      "Epoch 64: 11.652035474777222 seconds\n",
      "Epoch 65: Train Loss: 0.036| Train Acc: 99.208 | Test Loss: 0.267 | Test Acc: 92.200\n",
      "Epoch 65: 12.10245966911316 seconds\n",
      "Epoch 66: Train Loss: 0.037| Train Acc: 99.1560 | Test Loss: 0.269 | Test Acc: 92.080\n",
      "Epoch 66: 12.332322120666504 seconds\n",
      "Epoch 67: Train Loss: 0.036| Train Acc: 99.216 | Test Loss: 0.267 | Test Acc: 92.230\n",
      "Epoch 67: 11.611015796661377 seconds\n",
      "Epoch 68: Train Loss: 0.034| Train Acc: 99.250 | Test Loss: 0.272 | Test Acc: 92.080\n",
      "Epoch 68: 11.505208492279053 seconds\n",
      "Epoch 69: Train Loss: 0.034| Train Acc: 99.268 | Test Loss: 0.270 | Test Acc: 92.150\n",
      "Epoch 69: 12.082388401031494 seconds\n",
      "Epoch 70: Train Loss: 0.034| Train Acc: 99.274 | Test Loss: 0.271 | Test Acc: 92.040\n",
      "Epoch 70: 11.422316551208496 seconds\n",
      "Epoch 71: Train Loss: 0.032| Train Acc: 99.3040 | Test Loss: 0.276 | Test Acc: 92.050\n",
      "Epoch 71: 11.578327178955078 seconds\n",
      "Epoch 72: Train Loss: 0.032| Train Acc: 99.316 | Test Loss: 0.275 | Test Acc: 91.980\n",
      "Epoch 72: 11.47751522064209 seconds\n",
      "Epoch 73: Train Loss: 0.031| Train Acc: 99.418 | Test Loss: 0.274 | Test Acc: 92.110\n",
      "Epoch 73: 11.813791751861572 seconds\n",
      "Epoch 74: Train Loss: 0.031| Train Acc: 99.348 | Test Loss: 0.278 | Test Acc: 92.000\n",
      "Epoch 74: 11.488902807235718 seconds\n",
      "Epoch 75: Train Loss: 0.030| Train Acc: 99.394 | Test Loss: 0.275 | Test Acc: 92.270\n",
      "Epoch 75: 11.840074300765991 seconds\n",
      "Epoch 76: Train Loss: 0.029| Train Acc: 99.4520 | Test Loss: 0.276 | Test Acc: 92.020\n",
      "Epoch 76: 11.723459243774414 seconds\n",
      "Epoch 77: Train Loss: 0.029| Train Acc: 99.4320 | Test Loss: 0.272 | Test Acc: 92.190\n",
      "Epoch 77: 11.380149364471436 seconds\n",
      "Epoch 78: Train Loss: 0.029| Train Acc: 99.4480 | Test Loss: 0.275 | Test Acc: 92.200\n",
      "Epoch 78: 11.531018018722534 seconds\n",
      "Epoch 79: Train Loss: 0.029| Train Acc: 99.392 | Test Loss: 0.274 | Test Acc: 92.170\n",
      "Epoch 79: 11.857280254364014 seconds\n",
      "Epoch 80: Train Loss: 0.029| Train Acc: 99.440 | Test Loss: 0.278 | Test Acc: 92.150\n",
      "Epoch 80: 12.028936386108398 seconds\n",
      "Epoch 81: Train Loss: 0.029| Train Acc: 99.4780 | Test Loss: 0.274 | Test Acc: 92.220\n",
      "Epoch 81: 12.189321994781494 seconds\n",
      "Epoch 82: Train Loss: 0.028| Train Acc: 99.510 | Test Loss: 0.276 | Test Acc: 92.310\n",
      "Epoch 82: 11.60904335975647 seconds\n",
      "Epoch 83: Train Loss: 0.029| Train Acc: 99.4500 | Test Loss: 0.275 | Test Acc: 92.280\n",
      "Epoch 83: 12.098835706710815 seconds\n",
      "Epoch 84: Train Loss: 0.028| Train Acc: 99.4760 | Test Loss: 0.274 | Test Acc: 92.200\n",
      "Epoch 84: 11.829407215118408 seconds\n",
      "Epoch 85: Train Loss: 0.028| Train Acc: 99.454 | Test Loss: 0.273 | Test Acc: 92.310\n",
      "Epoch 85: 12.178508758544922 seconds\n",
      "Epoch 86: Train Loss: 0.029| Train Acc: 99.452 | Test Loss: 0.274 | Test Acc: 92.250\n",
      "Epoch 86: 12.04280948638916 seconds\n",
      "Epoch 87: Train Loss: 0.028| Train Acc: 99.476 | Test Loss: 0.274 | Test Acc: 92.160\n",
      "Epoch 87: 12.400766611099243 seconds\n",
      "Epoch 88: Train Loss: 0.028| Train Acc: 99.4620 | Test Loss: 0.276 | Test Acc: 92.190\n",
      "Epoch 88: 11.781490802764893 seconds\n",
      "Epoch 89: Train Loss: 0.029| Train Acc: 99.4100 | Test Loss: 0.275 | Test Acc: 92.310\n",
      "Epoch 89: 11.51910400390625 seconds\n",
      "Epoch 90: Train Loss: 0.028| Train Acc: 99.4960 | Test Loss: 0.274 | Test Acc: 92.220\n",
      "Epoch 90: 11.761638402938843 seconds\n",
      "Epoch 91: Train Loss: 0.028| Train Acc: 99.486 | Test Loss: 0.273 | Test Acc: 92.260\n",
      "Epoch 91: 12.437012434005737 seconds\n",
      "Epoch 92: Train Loss: 0.027| Train Acc: 99.4800 | Test Loss: 0.276 | Test Acc: 92.170\n",
      "Epoch 92: 11.981549501419067 seconds\n",
      "Epoch 93: Train Loss: 0.028| Train Acc: 99.458 | Test Loss: 0.274 | Test Acc: 92.230\n",
      "Epoch 93: 11.610481977462769 seconds\n",
      "Epoch 94: Train Loss: 0.027| Train Acc: 99.486 | Test Loss: 0.273 | Test Acc: 92.190\n",
      "Epoch 94: 12.533041715621948 seconds\n",
      "Epoch 95: Train Loss: 0.027| Train Acc: 99.5140 | Test Loss: 0.274 | Test Acc: 92.280\n",
      "Epoch 95: 11.451961040496826 seconds\n",
      "Epoch 96: Train Loss: 0.028| Train Acc: 99.448 | Test Loss: 0.274 | Test Acc: 92.220\n",
      "Epoch 96: 12.265256643295288 seconds\n",
      "Epoch 97: Train Loss: 0.028| Train Acc: 99.5020 | Test Loss: 0.273 | Test Acc: 92.190\n",
      "Epoch 97: 12.006529092788696 seconds\n",
      "Epoch 98: Train Loss: 0.027| Train Acc: 99.468 | Test Loss: 0.274 | Test Acc: 92.270\n",
      "Epoch 98: 11.329545974731445 seconds\n",
      "Epoch 99: Train Loss: 0.027| Train Acc: 99.544 | Test Loss: 0.271 | Test Acc: 92.380\n",
      "Epoch 99: 12.228528261184692 seconds\n",
      "Total training time: 1180.1648783683777 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(43) # to give stable randomness \n",
    "\n",
    "device = 'cuda'\n",
    "net = ConvNet()\n",
    "net = net.to(device)\n",
    "\n",
    "# PART 1.1: set the learning rate (lr) used in the optimizer.\n",
    "lr = 0.1    # Part 1.1 results tell us this has the best test accuracy out of the three \n",
    "\n",
    "# PART 1.1: Modify this to train for a short 5 epochs\n",
    "# PART 1.2: Modify this to train a longer 100 epochs\n",
    "epochs = 100\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9,\n",
    "                            weight_decay=5e-4)\n",
    "\n",
    "# PART 1.2: try different learning rate scheduler \n",
    "scheduler_name= 'multistep'   # set this to 'multistep' or 'cosine_annealing' (or None for Part 1.1)\n",
    "if scheduler_name=='multistep':\n",
    "    milestones = [25, 50, 75, 100]\n",
    "    gamma      = 0.1\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                    milestones=milestones,\n",
    "                                                    gamma=gamma)\n",
    "elif scheduler_name=='cosine_annealing':\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    milestones = []\n",
    "\n",
    "elif scheduler_name==None:\n",
    "    milestones = []; gamma = 0\n",
    "\n",
    "else:\n",
    "    NotImplementedError\n",
    "\n",
    "# Records the training loss and training accuracy during training\n",
    "train_loss_tracker, train_acc_tracker = [], []\n",
    "\n",
    "# Records the test loss and test accuracy during training\n",
    "test_loss_tracker, test_acc_tracker = [], []\n",
    "\n",
    "print('Training for {} epochs, with learning rate {} and milestones {}'.format(\n",
    "      epochs, lr, milestones))\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(0, epochs):\n",
    "    ep_start_time = time.time()\n",
    "    \n",
    "    train(net, epoch, train_loss_tracker, train_acc_tracker)\n",
    "    test(net, epoch, test_loss_tracker, test_acc_tracker)\n",
    "    scheduler.step()\n",
    "    \n",
    "    ep_end_time = time.time()\n",
    "    epoch_time = ep_end_time - ep_start_time\n",
    "    print(f\"Epoch {epoch}: {epoch_time} seconds\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print('Total training time: {} seconds'.format(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "M = net.modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "m = next(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gradient": {},
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_get_backward_hooks',\n",
       " '_get_name',\n",
       " '_is_full_backward_hook',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'classifier',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'model',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for m in net.modules():\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        w = m.weight; b = m.bias\n",
    "        W = w\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "gradient": {},
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.2592e-04, -5.7475e-05, -4.7485e-04],\n",
       "          [-7.8162e-04, -1.2203e-03, -1.5978e-03],\n",
       "          [-7.9703e-04, -1.3087e-03, -1.9508e-03]],\n",
       "\n",
       "         [[ 1.9689e-04,  3.7990e-05, -4.2284e-04],\n",
       "          [-8.3158e-04, -1.2807e-03, -1.6360e-03],\n",
       "          [-9.3918e-04, -1.4063e-03, -1.9979e-03]],\n",
       "\n",
       "         [[ 9.3454e-04,  7.5219e-04,  3.4766e-04],\n",
       "          [-1.3931e-04, -5.8620e-04, -8.7297e-04],\n",
       "          [-3.5968e-04, -7.1853e-04, -1.2178e-03]]],\n",
       "\n",
       "\n",
       "        [[[-4.2590e-03, -1.9439e-03,  5.2018e-04],\n",
       "          [-2.9612e-03, -1.2428e-05,  2.3305e-03],\n",
       "          [-4.0652e-03, -1.4808e-03,  9.1480e-04]],\n",
       "\n",
       "         [[-2.9791e-03, -1.8602e-03, -9.3304e-04],\n",
       "          [-1.8971e-03, -2.8362e-04,  1.8103e-04],\n",
       "          [-3.1598e-03, -1.2055e-03, -4.1428e-04]],\n",
       "\n",
       "         [[-4.8152e-04,  1.5994e-03,  2.8851e-03],\n",
       "          [ 4.7891e-04,  4.0578e-03,  5.1998e-03],\n",
       "          [-1.5172e-04,  3.4703e-03,  5.2405e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.4263e-04, -2.6837e-03, -4.2197e-03],\n",
       "          [-3.9340e-03, -6.2605e-03, -7.0310e-03],\n",
       "          [-2.8058e-03, -4.8990e-03, -5.9746e-03]],\n",
       "\n",
       "         [[ 2.2116e-03, -4.1035e-04, -1.9710e-03],\n",
       "          [-1.2617e-03, -3.6726e-03, -4.6338e-03],\n",
       "          [-5.6766e-04, -2.6158e-03, -3.8161e-03]],\n",
       "\n",
       "         [[ 3.6199e-03, -4.8073e-05, -2.6779e-03],\n",
       "          [ 1.2113e-03, -2.1778e-03, -4.1530e-03],\n",
       "          [ 1.3637e-03, -1.3013e-03, -3.0827e-03]]],\n",
       "\n",
       "\n",
       "        [[[-4.7059e-03,  2.7336e-03,  1.3532e-02],\n",
       "          [ 2.5149e-03,  1.2473e-02,  1.7173e-02],\n",
       "          [ 3.3071e-03,  4.1210e-03,  9.1389e-03]],\n",
       "\n",
       "         [[ 4.2287e-03, -1.6547e-03,  3.3683e-03],\n",
       "          [ 2.1213e-02,  1.9322e-02,  1.4983e-02],\n",
       "          [ 1.7073e-02,  1.1739e-02,  4.5992e-03]],\n",
       "\n",
       "         [[-3.1286e-02, -2.8687e-02, -1.4687e-02],\n",
       "          [-1.9123e-02, -7.1285e-03,  1.0611e-03],\n",
       "          [-1.5567e-02, -1.9973e-02, -1.6078e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.9060e-03, -1.7989e-03, -1.8782e-03],\n",
       "          [-1.4357e-03, -1.5680e-03, -1.4483e-03],\n",
       "          [-1.3297e-04,  3.5680e-05,  3.9358e-04]],\n",
       "\n",
       "         [[-1.4269e-03, -1.2706e-03, -1.2696e-03],\n",
       "          [-9.3149e-04, -1.0237e-03, -8.1522e-04],\n",
       "          [-4.5701e-05,  2.3263e-04,  7.4381e-04]],\n",
       "\n",
       "         [[-1.9109e-03, -1.7052e-03, -1.7061e-03],\n",
       "          [-1.3185e-03, -1.4498e-03, -1.3290e-03],\n",
       "          [-4.2115e-04, -3.1013e-04,  1.2661e-04]]],\n",
       "\n",
       "\n",
       "        [[[-8.3106e-03, -1.0578e-02, -1.0971e-02],\n",
       "          [-5.2455e-03, -7.7452e-03, -7.8105e-03],\n",
       "          [-4.9668e-03, -6.2090e-03, -5.6850e-03]],\n",
       "\n",
       "         [[-2.9303e-03, -3.6983e-03, -2.5713e-03],\n",
       "          [-3.8256e-04, -1.3082e-03, -2.4362e-04],\n",
       "          [ 1.2225e-03,  1.4052e-03,  2.6371e-03]],\n",
       "\n",
       "         [[ 8.2224e-04, -1.2335e-03, -7.1645e-04],\n",
       "          [ 4.0989e-03,  1.8397e-03,  2.2753e-03],\n",
       "          [ 5.4722e-03,  4.8266e-03,  5.6324e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.3396e-03, -8.9954e-04, -8.8693e-04],\n",
       "          [ 1.7251e-03, -1.2266e-03, -6.3631e-04],\n",
       "          [-1.2143e-03, -3.9808e-03, -4.0855e-03]],\n",
       "\n",
       "         [[ 2.5455e-03, -9.2917e-04, -1.1382e-03],\n",
       "          [ 1.9398e-03, -1.2652e-03, -7.7132e-04],\n",
       "          [-1.0084e-03, -3.9226e-03, -3.9116e-03]],\n",
       "\n",
       "         [[ 1.8064e-03, -1.1450e-03, -1.4308e-03],\n",
       "          [ 1.0505e-03, -1.3036e-03, -7.2924e-04],\n",
       "          [-2.2981e-03, -4.2820e-03, -4.1014e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.0346e-03,  2.8813e-03,  2.8651e-03],\n",
       "          [ 4.0466e-03,  4.2699e-03,  3.4630e-03],\n",
       "          [ 7.3567e-03,  7.4471e-03,  6.8564e-03]],\n",
       "\n",
       "         [[-6.4947e-04,  3.2596e-04,  3.7287e-04],\n",
       "          [ 1.6473e-03,  1.9327e-03,  1.1900e-03],\n",
       "          [ 5.0938e-03,  5.1795e-03,  4.7233e-03]],\n",
       "\n",
       "         [[-1.9377e-03, -4.3082e-04,  1.3267e-05],\n",
       "          [ 5.9911e-05,  9.0557e-04,  6.7022e-04],\n",
       "          [ 3.6045e-03,  4.3444e-03,  4.4870e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 6.8699e-03,  7.4648e-03,  1.8405e-02],\n",
       "          [ 1.9425e-03,  3.5019e-04,  2.1327e-03],\n",
       "          [-6.9701e-03, -6.7859e-03,  6.8989e-04]],\n",
       "\n",
       "         [[-1.3327e-02, -5.3261e-03,  9.7296e-03],\n",
       "          [-8.3980e-03, -2.8506e-03,  3.4209e-03],\n",
       "          [-1.8204e-02, -9.1539e-03,  4.2171e-03]],\n",
       "\n",
       "         [[-1.5977e-02, -7.2174e-03, -2.0364e-03],\n",
       "          [-1.3209e-02, -1.3420e-03,  1.3139e-03],\n",
       "          [-2.4770e-02, -1.7795e-02, -9.9844e-04]]],\n",
       "\n",
       "\n",
       "        [[[-4.5246e-03, -7.5628e-03, -1.4127e-02],\n",
       "          [-9.6992e-03, -1.1552e-02, -1.3819e-02],\n",
       "          [-9.1411e-03, -8.5595e-03, -7.9691e-03]],\n",
       "\n",
       "         [[-4.9888e-03, -4.6117e-03, -7.4051e-03],\n",
       "          [-6.3160e-03, -5.4632e-03, -5.2493e-03],\n",
       "          [-4.3493e-03, -1.3292e-03,  4.2171e-04]],\n",
       "\n",
       "         [[ 6.6542e-03,  4.1944e-03,  1.0501e-04],\n",
       "          [ 5.6421e-03,  5.8644e-03,  4.7803e-03],\n",
       "          [ 7.2413e-03,  1.1675e-02,  1.3855e-02]]],\n",
       "\n",
       "\n",
       "        [[[-5.4632e-03, -8.2679e-03, -3.9424e-03],\n",
       "          [ 2.2639e-04, -2.7145e-03,  1.2550e-03],\n",
       "          [-6.7284e-04, -2.6459e-03,  1.9528e-03]],\n",
       "\n",
       "         [[-6.3861e-03, -9.2318e-03, -5.0420e-03],\n",
       "          [-1.4249e-03, -4.4254e-03, -7.8796e-04],\n",
       "          [-1.9781e-03, -4.1839e-03, -9.0171e-06]],\n",
       "\n",
       "         [[-5.3492e-03, -8.9948e-03, -6.6790e-03],\n",
       "          [-3.6218e-04, -4.1653e-03, -2.3216e-03],\n",
       "          [-1.0677e-03, -3.8859e-03, -1.3107e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.6899e-03, -2.2738e-03, -1.7976e-03],\n",
       "          [ 8.4862e-04,  1.2059e-03,  5.9114e-04],\n",
       "          [ 2.2739e-03,  7.6506e-04, -6.9823e-04]],\n",
       "\n",
       "         [[-4.4910e-04, -1.5100e-04,  1.4188e-03],\n",
       "          [ 1.4179e-03,  2.4960e-03,  4.7946e-03],\n",
       "          [ 4.2666e-03,  2.9273e-03,  4.0785e-03]],\n",
       "\n",
       "         [[ 3.4032e-04, -9.6555e-04,  1.4674e-03],\n",
       "          [ 2.7701e-03,  1.0354e-03,  3.1827e-03],\n",
       "          [ 7.1774e-03,  3.5394e-03,  2.9981e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.4812e-02, -2.1956e-02,  9.4246e-03],\n",
       "          [-3.8609e-02, -2.0959e-02,  9.4627e-03],\n",
       "          [-3.8761e-02, -2.4556e-02, -3.3853e-03]],\n",
       "\n",
       "         [[-2.2507e-02, -1.4273e-02,  1.6906e-02],\n",
       "          [-3.2342e-02, -1.3361e-02,  1.1404e-02],\n",
       "          [-2.6118e-02, -1.4404e-02, -1.8193e-03]],\n",
       "\n",
       "         [[-2.4747e-03,  1.4189e-02,  4.8755e-02],\n",
       "          [-1.1691e-02,  1.9420e-02,  4.7786e-02],\n",
       "          [-9.6121e-03,  7.2722e-03,  2.1180e-02]]],\n",
       "\n",
       "\n",
       "        [[[-2.9479e-03, -3.7834e-03, -9.0346e-03],\n",
       "          [-1.4407e-02, -1.0672e-02, -1.1545e-02],\n",
       "          [-1.2853e-02, -9.7773e-03, -6.4514e-03]],\n",
       "\n",
       "         [[ 2.9296e-04,  8.8545e-04, -5.7832e-03],\n",
       "          [-5.4000e-03, -1.9341e-03, -3.9227e-03],\n",
       "          [-7.0673e-03, -6.1244e-03, -4.3650e-03]],\n",
       "\n",
       "         [[ 1.1007e-02,  9.4161e-03,  3.5503e-03],\n",
       "          [ 5.9767e-03,  9.4683e-03,  7.6476e-03],\n",
       "          [ 4.3642e-03,  3.8012e-03,  3.7371e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 7.9103e-04, -1.8560e-04, -2.0069e-03],\n",
       "          [-1.1277e-03, -3.5877e-03, -5.0499e-03],\n",
       "          [ 3.7672e-03,  1.9141e-03,  5.3708e-04]],\n",
       "\n",
       "         [[ 1.5474e-03,  4.2435e-04, -2.9691e-03],\n",
       "          [-1.6608e-03, -4.1100e-03, -5.7484e-03],\n",
       "          [ 1.8621e-03, -9.8815e-04, -2.2537e-03]],\n",
       "\n",
       "         [[-1.5526e-03, -2.8907e-03, -5.0925e-03],\n",
       "          [-3.5634e-03, -8.1200e-03, -9.9994e-03],\n",
       "          [-4.6269e-04, -4.6269e-03, -6.5849e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 1.9903e-03,  2.6889e-03,  4.0034e-03],\n",
       "          [ 3.6452e-03,  3.0354e-03,  3.0237e-03],\n",
       "          [ 8.4576e-03,  7.4006e-03,  6.7534e-03]],\n",
       "\n",
       "         [[ 2.5937e-03,  4.1129e-03,  5.5022e-03],\n",
       "          [ 4.4967e-03,  4.2890e-03,  4.2693e-03],\n",
       "          [ 8.1090e-03,  7.6687e-03,  7.2949e-03]],\n",
       "\n",
       "         [[ 9.4460e-04,  4.8382e-03,  7.7086e-03],\n",
       "          [ 3.4158e-03,  5.2356e-03,  6.8196e-03],\n",
       "          [ 6.9655e-03,  8.0824e-03,  9.5006e-03]]],\n",
       "\n",
       "\n",
       "        [[[-5.0241e-04,  8.8628e-04,  7.1974e-04],\n",
       "          [ 3.4376e-03,  3.3469e-03,  8.0901e-04],\n",
       "          [ 8.7803e-03,  7.6932e-03,  3.9301e-03]],\n",
       "\n",
       "         [[-4.3507e-03, -3.1596e-03, -2.5912e-03],\n",
       "          [ 9.8587e-04,  5.1581e-04, -1.5609e-03],\n",
       "          [ 6.1270e-03,  4.4965e-03,  8.8173e-04]],\n",
       "\n",
       "         [[-2.3712e-03,  1.7140e-03,  3.6444e-03],\n",
       "          [ 1.7522e-03,  3.7410e-03,  3.1950e-03],\n",
       "          [ 6.3874e-03,  6.3692e-03,  4.2332e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.9489e-03,  3.6861e-04,  1.7843e-03],\n",
       "          [-3.0132e-04,  2.0615e-03,  3.0910e-03],\n",
       "          [-4.4865e-04,  8.4693e-04,  1.0907e-03]],\n",
       "\n",
       "         [[-4.1371e-03, -1.4576e-03,  2.9327e-04],\n",
       "          [-1.7998e-03,  9.8342e-04,  2.3479e-03],\n",
       "          [-1.6162e-03,  1.2849e-04,  7.4675e-04]],\n",
       "\n",
       "         [[-4.1896e-03, -3.5619e-04,  1.9271e-03],\n",
       "          [-1.7751e-03,  1.9904e-03,  3.8192e-03],\n",
       "          [-1.3915e-03,  1.0053e-03,  1.9883e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.4810e-03,  2.6820e-03,  7.1687e-03],\n",
       "          [-4.5203e-03,  3.1665e-03,  4.5066e-03],\n",
       "          [-2.1563e-02, -1.7865e-02, -1.5995e-02]],\n",
       "\n",
       "         [[ 1.9203e-03,  2.5715e-03,  3.5293e-03],\n",
       "          [-1.5179e-03,  4.1698e-03,  3.0337e-03],\n",
       "          [-2.0263e-02, -1.4834e-02, -1.2849e-02]],\n",
       "\n",
       "         [[ 1.8551e-03,  3.5512e-03,  2.7747e-03],\n",
       "          [-3.1156e-03,  6.0787e-03,  7.5803e-03],\n",
       "          [-2.1792e-02, -1.6014e-02, -1.1971e-02]]],\n",
       "\n",
       "\n",
       "        [[[-1.8520e-02, -2.0301e-02, -2.0312e-02],\n",
       "          [-1.5071e-02, -1.7988e-02, -1.9948e-02],\n",
       "          [-1.5427e-02, -1.8018e-02, -2.0674e-02]],\n",
       "\n",
       "         [[-1.3299e-02, -1.5236e-02, -1.3813e-02],\n",
       "          [-1.1218e-02, -1.3612e-02, -1.3207e-02],\n",
       "          [-1.1107e-02, -1.3031e-02, -1.3431e-02]],\n",
       "\n",
       "         [[-6.3851e-03, -8.5510e-03, -6.4309e-03],\n",
       "          [-4.8199e-03, -7.1967e-03, -6.2940e-03],\n",
       "          [-4.1922e-03, -6.3122e-03, -6.3008e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 5.0979e-03,  3.3039e-03,  4.0703e-03],\n",
       "          [ 6.5131e-03,  4.1893e-03,  5.6576e-03],\n",
       "          [ 5.0075e-03,  2.0691e-03,  2.7361e-03]],\n",
       "\n",
       "         [[ 1.4616e-03,  8.5287e-05,  9.1528e-05],\n",
       "          [ 3.0128e-03,  1.2596e-03,  1.8062e-03],\n",
       "          [ 1.1249e-03, -1.2315e-03, -1.2361e-03]],\n",
       "\n",
       "         [[-2.0343e-03, -2.3251e-03, -4.0954e-03],\n",
       "          [ 6.5751e-05, -2.1140e-04, -1.5656e-03],\n",
       "          [-1.9659e-03, -2.5540e-03, -4.1445e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 3.9857e-03,  1.3167e-03,  3.8616e-03],\n",
       "          [ 3.2328e-03,  1.7502e-03,  4.2600e-03],\n",
       "          [ 2.9989e-03,  1.1388e-03,  2.3083e-03]],\n",
       "\n",
       "         [[ 5.2434e-03,  2.6286e-03,  5.2775e-03],\n",
       "          [ 4.5265e-03,  2.8518e-03,  5.4729e-03],\n",
       "          [ 4.0122e-03,  1.8820e-03,  3.1407e-03]],\n",
       "\n",
       "         [[ 2.1394e-03,  1.6280e-03,  5.1259e-03],\n",
       "          [ 8.3029e-04,  1.3817e-03,  5.2025e-03],\n",
       "          [ 1.4991e-04,  5.3845e-05,  2.5354e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 2.4942e-04, -2.6453e-03, -4.7533e-03],\n",
       "          [ 2.6109e-03, -1.2079e-03, -3.6160e-03],\n",
       "          [ 6.9300e-03,  2.7778e-03,  2.1402e-04]],\n",
       "\n",
       "         [[-4.0236e-03, -7.2150e-03, -1.0609e-02],\n",
       "          [-4.1174e-03, -7.3502e-03, -9.9903e-03],\n",
       "          [-2.6278e-03, -4.6615e-03, -5.9868e-03]],\n",
       "\n",
       "         [[-5.9072e-03, -8.5249e-03, -1.1703e-02],\n",
       "          [-5.0167e-03, -7.7616e-03, -1.0211e-02],\n",
       "          [-2.7792e-03, -4.5597e-03, -5.6159e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 7.4868e-03,  1.5188e-02,  1.6479e-02],\n",
       "          [ 9.8453e-03,  1.6717e-02,  1.9848e-02],\n",
       "          [-8.5476e-03, -1.5969e-03,  6.2926e-03]],\n",
       "\n",
       "         [[ 3.5024e-04, -4.6343e-03, -5.2520e-03],\n",
       "          [-2.2491e-03, -8.2456e-03, -7.7505e-03],\n",
       "          [-1.3673e-02, -1.7941e-02, -1.2649e-02]],\n",
       "\n",
       "         [[-1.9434e-02, -2.2552e-02, -2.3562e-02],\n",
       "          [-2.2673e-02, -2.3389e-02, -2.0488e-02],\n",
       "          [-3.2405e-02, -3.1345e-02, -2.0261e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.0220e-04,  1.8543e-03, -2.6071e-03],\n",
       "          [ 7.3547e-03,  7.3447e-03,  2.3262e-03],\n",
       "          [ 6.7848e-03,  8.6330e-03,  3.2479e-03]],\n",
       "\n",
       "         [[ 5.9060e-03,  8.7126e-03,  9.2242e-03],\n",
       "          [ 1.2506e-02,  1.3458e-02,  1.0743e-02],\n",
       "          [ 1.0880e-02,  1.6821e-02,  1.3225e-02]],\n",
       "\n",
       "         [[ 9.3659e-03,  1.1396e-02,  1.0807e-02],\n",
       "          [ 1.9644e-02,  1.7991e-02,  1.1477e-02],\n",
       "          [ 1.7409e-02,  2.2329e-02,  1.7818e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 4.2269e-03, -3.5631e-02, -2.1906e-02],\n",
       "          [-9.8045e-03, -4.1721e-02, -4.2348e-02],\n",
       "          [ 2.2552e-04, -5.1121e-02, -5.3089e-02]],\n",
       "\n",
       "         [[ 4.7518e-03, -2.6968e-02, -2.5966e-02],\n",
       "          [-7.7647e-03, -3.0604e-02, -3.4976e-02],\n",
       "          [-3.7762e-03, -4.1444e-02, -3.7027e-02]],\n",
       "\n",
       "         [[ 1.3373e-02, -2.2614e-02, -2.5758e-02],\n",
       "          [-1.0872e-02, -3.6002e-02, -3.4447e-02],\n",
       "          [ 7.8155e-03, -4.2841e-02, -4.7335e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1098e-02,  1.1467e-02,  1.3674e-02],\n",
       "          [ 1.1532e-02,  1.3634e-02,  9.6516e-03],\n",
       "          [ 1.3347e-02,  1.3727e-02,  5.9153e-03]],\n",
       "\n",
       "         [[ 7.5080e-03,  8.1625e-03,  9.4378e-03],\n",
       "          [ 5.0847e-03,  6.3351e-03,  2.4804e-03],\n",
       "          [ 5.8887e-03,  3.0029e-03, -3.4066e-03]],\n",
       "\n",
       "         [[-4.4763e-03, -1.1375e-03,  7.4420e-04],\n",
       "          [-7.0811e-03, -4.6646e-03, -5.8150e-03],\n",
       "          [-4.3226e-03, -7.5352e-03, -1.3351e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 8.8230e-04,  3.4637e-04,  1.8761e-04],\n",
       "          [ 7.0755e-04,  2.2829e-04, -3.9484e-05],\n",
       "          [ 7.3765e-04,  3.8638e-04, -2.0816e-06]],\n",
       "\n",
       "         [[ 9.8747e-04,  6.7376e-04,  4.3346e-04],\n",
       "          [ 7.7336e-04,  6.1324e-04,  3.6719e-04],\n",
       "          [ 7.0093e-04,  4.8121e-04,  2.6793e-04]],\n",
       "\n",
       "         [[ 2.0182e-03,  1.5975e-03,  1.2191e-03],\n",
       "          [ 1.7960e-03,  1.5481e-03,  1.2476e-03],\n",
       "          [ 1.6492e-03,  1.3385e-03,  1.1486e-03]]],\n",
       "\n",
       "\n",
       "        [[[-1.0352e-03, -6.4487e-04, -7.0088e-04],\n",
       "          [-1.7277e-03, -7.1854e-04, -1.4318e-03],\n",
       "          [-2.3123e-03, -1.9952e-03, -2.2565e-03]],\n",
       "\n",
       "         [[-1.5843e-03, -1.0160e-03, -9.9314e-04],\n",
       "          [-1.8413e-03, -6.0706e-04, -1.2876e-03],\n",
       "          [-2.1528e-03, -1.7060e-03, -1.9361e-03]],\n",
       "\n",
       "         [[-1.1683e-03, -4.0430e-04, -3.4131e-04],\n",
       "          [-1.7205e-03, -2.8859e-04, -9.0931e-04],\n",
       "          [-2.0464e-03, -1.5053e-03, -1.6897e-03]]],\n",
       "\n",
       "\n",
       "        [[[ 3.5872e-04, -1.7350e-03, -2.6728e-03],\n",
       "          [ 9.2757e-04, -1.4874e-03, -2.7306e-03],\n",
       "          [-1.1568e-03, -4.0431e-03, -4.2995e-03]],\n",
       "\n",
       "         [[-2.4707e-03, -3.0450e-03, -2.9634e-03],\n",
       "          [-2.6894e-03, -3.1161e-03, -3.2553e-03],\n",
       "          [-4.7209e-03, -5.5113e-03, -4.6272e-03]],\n",
       "\n",
       "         [[-1.5095e-03, -3.1297e-03, -3.8769e-03],\n",
       "          [-1.4049e-03, -2.3042e-03, -2.5250e-03],\n",
       "          [-3.5473e-03, -4.6862e-03, -3.4067e-03]]],\n",
       "\n",
       "\n",
       "        [[[-2.6268e-03, -6.9049e-03, -4.3226e-03],\n",
       "          [ 2.3702e-03, -1.4165e-03, -9.2748e-04],\n",
       "          [ 1.6636e-04, -3.6170e-03, -3.9780e-03]],\n",
       "\n",
       "         [[-1.8945e-03, -5.6647e-03, -2.4852e-03],\n",
       "          [ 2.0650e-03, -1.2616e-03,  3.5117e-05],\n",
       "          [ 1.6282e-04, -3.1673e-03, -3.0843e-03]],\n",
       "\n",
       "         [[ 7.4478e-04, -2.5050e-03, -3.4314e-04],\n",
       "          [ 5.1540e-03,  2.4274e-03,  2.0435e-03],\n",
       "          [ 2.7313e-03, -1.8855e-04, -1.3830e-03]]],\n",
       "\n",
       "\n",
       "        [[[-5.5561e-03, -8.2539e-03, -1.1960e-02],\n",
       "          [-3.6314e-03, -3.7564e-03, -6.0619e-03],\n",
       "          [-5.8351e-03, -7.1513e-03, -8.3871e-03]],\n",
       "\n",
       "         [[-2.9209e-03, -3.6190e-03, -8.2145e-03],\n",
       "          [-2.2120e-03,  2.0212e-04, -1.5975e-03],\n",
       "          [-4.5039e-03, -3.4736e-03, -2.7059e-03]],\n",
       "\n",
       "         [[ 1.2573e-02,  8.3020e-03,  3.3961e-03],\n",
       "          [ 1.4586e-02,  1.1732e-02,  8.5567e-03],\n",
       "          [ 1.4074e-02,  1.0633e-02,  8.0230e-03]]]], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 3, 3])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 3, 3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
